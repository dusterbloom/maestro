#!/usr/bin/env python3
"""
Critical Fixes Validation Script

This script tests all the critical fixes implemented to ensure:
1. STT service processes incomplete segments correctly
2. WebSocket connections work properly
3. Audio pipeline functions end-to-end
4. Memory/embedding services handle errors gracefully

Usage:
    python scripts/test-critical-fixes.py
"""

import asyncio
import json
import time
import logging
import websockets
import base64
import numpy as np
from pathlib import Path
import sys
import os

# Add the orchestrator src to path for imports
sys.path.append(str(Path(__file__).parent.parent / "orchestrator" / "src"))

from config import config
from services.stt_service import STTService
from services.memory_service import MemoryService
from services.voice_service import VoiceService
from core.state_machine import StateMachine
from core.event_dispatcher import EventDispatcher

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CriticalFixesValidator:
    def __init__(self):
        self.test_results = {}
        self.session_id = f"test_session_{int(time.time())}"
        
    async def run_all_tests(self):
        """Run all critical validation tests"""
        logger.info("üß™ Starting Critical Fixes Validation")
        
        tests = [
            ("STT Segment Processing", self.test_stt_segment_processing),
            ("WebSocket Configuration", self.test_websocket_config),
            ("Memory Service Initialization", self.test_memory_service),
            ("Voice Service Embedding", self.test_voice_service),
            ("End-to-End Audio Pipeline", self.test_audio_pipeline),
        ]
        
        for test_name, test_func in tests:
            logger.info(f"üß™ Running test: {test_name}")
            try:
                result = await test_func()
                self.test_results[test_name] = {"status": "PASS", "details": result}
                logger.info(f"‚úÖ {test_name}: PASSED")
            except Exception as e:
                self.test_results[test_name] = {"status": "FAIL", "error": str(e)}
                logger.error(f"‚ùå {test_name}: FAILED - {e}")
        
        self.print_summary()
    
    async def test_stt_segment_processing(self):
        """Test that STT service processes incomplete segments correctly"""
        logger.info("Testing STT segment processing with incomplete segments...")
        
        received_events = []
        
        async def mock_event_callback(event):
            received_events.append(event)
            logger.info(f"Mock callback received: {event}")
        
        stt_service = STTService(self.session_id, mock_event_callback)
        
        # Simulate incomplete segments (matching the log patterns)
        test_segments = [
            [{'start': '0.270', 'end': '1.521', 'text': ' Hello there!', 'completed': False}],
            [{'start': '0.270', 'end': '1.752', 'text': ' Hello there, can you?', 'completed': False}],
            [{'start': '0.270', 'end': '2.100', 'text': ' Hello there, can you hear me?', 'completed': False}],
        ]
        
        # Process segments with delays to simulate real-time behavior
        for i, segments in enumerate(test_segments):
            logger.info(f"Processing segment {i+1}: {segments[0]['text']}")
            await stt_service._process_segments_intelligently(segments)
            await asyncio.sleep(0.5)  # Small delay between segments
        
        # Wait for segment completion timer
        await asyncio.sleep(2.0)
        
        # Verify that we received a transcript.final event
        final_events = [e for e in received_events if e.get('type') == 'transcript.final']
        
        if not final_events:
            raise AssertionError("No transcript.final event received")
            
        final_transcript = final_events[0]['data']['transcript']
        logger.info(f"Final transcript: '{final_transcript}'")
        
        if "Hello there, can you hear me?" not in final_transcript:
            raise AssertionError(f"Expected complete transcript, got: '{final_transcript}'")
            
        return {"transcript": final_transcript, "events_count": len(received_events)}
    
    async def test_websocket_config(self):
        """Test WebSocket URL configuration"""
        logger.info(f"Testing WebSocket configuration: {config.WHISPER_URL}")
        
        if not config.WHISPER_URL.startswith("ws://"):
            raise AssertionError(f"WHISPER_URL should start with ws://, got: {config.WHISPER_URL}")
        
        # Try to connect to see if URL is reachable (may timeout, that's ok)
        try:
            async with asyncio.timeout(5):
                websocket = await websockets.connect(config.WHISPER_URL)
                await websocket.close()
                connection_status = "reachable"
        except Exception as e:
            connection_status = f"not_reachable: {e}"
            logger.warning(f"WebSocket not reachable (expected in testing): {e}")
        
        return {"url": config.WHISPER_URL, "format": "correct", "connection": connection_status}
    
    async def test_memory_service(self):
        """Test memory service initialization and error handling"""
        logger.info("Testing memory service initialization...")
        
        memory_service = MemoryService()
        
        try:
            # This will likely fail in testing, but should handle errors gracefully
            await memory_service.initialize_chroma_client()
            status = "connected"
        except Exception as e:
            # This is expected in testing environment
            status = f"graceful_failure: {e}"
            logger.info(f"Memory service failed gracefully (expected): {e}")
        
        return {"status": status, "config": {"redis_url": config.REDIS_URL, "chromadb_url": config.CHROMADB_URL}}
    
    async def test_voice_service(self):
        """Test voice service error handling"""
        logger.info("Testing voice service...")
        
        voice_service = VoiceService()
        
        # Test with dummy audio data
        dummy_audio = np.random.rand(1000).astype(np.float32).tobytes()
        
        try:
            embedding = await voice_service.get_embedding(dummy_audio)
            if embedding:
                status = "working"
            else:
                status = "returned_none"
        except Exception as e:
            status = f"error_handled: {e}"
            logger.info(f"Voice service error handled gracefully: {e}")
        
        return {"status": status, "device": config.RESEMBLYZER_DEVICE}
    
    async def test_audio_pipeline(self):
        """Test end-to-end audio pipeline simulation"""
        logger.info("Testing audio pipeline...")
        
        # Create minimal pipeline components
        state_machine = StateMachine()
        event_dispatcher = EventDispatcher()
        
        session = state_machine.get_or_create_session(self.session_id)
        
        # Test audio format handling
        test_audio_data = b"fake_wav_data" * 100  # Simulate audio data
        base64_audio = base64.b64encode(test_audio_data).decode('utf-8')
        
        # Verify base64 encoding/decoding works
        decoded_audio = base64.b64decode(base64_audio)
        
        if decoded_audio != test_audio_data:
            raise AssertionError("Base64 audio encoding/decoding failed")
        
        return {
            "base64_encoding": "working",
            "session_created": session.session_id,
            "audio_size": len(test_audio_data)
        }
    
    def print_summary(self):
        """Print test results summary"""
        logger.info("\n" + "="*50)
        logger.info("üß™ CRITICAL FIXES VALIDATION SUMMARY")
        logger.info("="*50)
        
        passed = sum(1 for r in self.test_results.values() if r["status"] == "PASS")
        total = len(self.test_results)
        
        for test_name, result in self.test_results.items():
            status_emoji = "‚úÖ" if result["status"] == "PASS" else "‚ùå"
            logger.info(f"{status_emoji} {test_name}: {result['status']}")
            
            if result["status"] == "FAIL":
                logger.info(f"   Error: {result.get('error', 'Unknown')}")
            else:
                details = result.get('details', {})
                if isinstance(details, dict):
                    for key, value in details.items():
                        logger.info(f"   {key}: {value}")
        
        logger.info("="*50)
        logger.info(f"üéØ Results: {passed}/{total} tests passed")
        
        if passed == total:
            logger.info("üéâ All critical fixes validated successfully!")
        else:
            logger.warning(f"‚ö†Ô∏è {total - passed} test(s) failed - review the issues above")
        
        return passed == total

async def main():
    """Main test runner"""
    validator = CriticalFixesValidator()
    await validator.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())
